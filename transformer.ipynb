{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def get_emb_matrix(target_i2w, w2v, EMB_DIM, unk_size=0.25, with_flags = True, oov_filename=None):\n",
    "    emb_mat = np.zeros((len(target_i2w), EMB_DIM))\n",
    "    notfound = []\n",
    "\n",
    "    if with_flags:\n",
    "        border = 2\n",
    "    else:\n",
    "        border = -1\n",
    "\n",
    "    for i in target_i2w.keys():\n",
    "        if i>border:\n",
    "            if target_i2w[i] in w2v:\n",
    "                emb_mat[i] = w2v[target_i2w[i]]\n",
    "            else:\n",
    "                notfound.append(target_i2w[i])\n",
    "                #print(target_i2w[i], \"not there!\")\n",
    "                emb_mat[i] = np.random.uniform(-unk_size, unk_size, EMB_DIM)\n",
    "\n",
    "    if oov_filename is not None:\n",
    "        with open(oov_filename, 'w') as fd:\n",
    "            for i in notfound:\n",
    "                fd.write(i+'\\n')\n",
    "\n",
    "    return emb_mat, notfound\n",
    "\n",
    "\n",
    "def load_embs(embfile):\n",
    "    with open(embfile,'r',encoding='utf8') as fd:\n",
    "        t = fd.readlines()\n",
    "    t = [i.strip() for i in t]\n",
    "    t = [i.split(' ') for i in t]\n",
    "    words = [i[0] for i in t]\n",
    "    vecs = [i[1:] for i in t]\n",
    "    vecs = [np.array([float(i) for i in vec]) for vec in vecs]\n",
    "    D = dict(zip(words, vecs))\n",
    "    return D\n",
    "\n",
    "def write_dict_to_json(filename, D):\n",
    "    print(\"Writing dictionary to \"+filename)\n",
    "    with open(filename, 'w', encoding='utf8') as f:\n",
    "        f.write(json.dumps(D, sort_keys=True, indent=4))\n",
    "\n",
    "def load_dict_from_json(filename):\n",
    "    with open(filename,'r',encoding='utf8') as f:\n",
    "        data = json.loads(f.read())\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(filename, numlines=None, optional_processing=False, revert_tgt=False, drop_cls=True):\n",
    "    \"\"\" filename: either str (name of file; the file must be a list of pairs\n",
    "        in the form src \\t tgt) or tuple (src, tgt).\n",
    "\n",
    "        If numlines is None, load all the data.\n",
    "\n",
    "        If optional_processing is True:\n",
    "            - lowercase everything\n",
    "            - remove punctuation\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(filename,str):\n",
    "        lines = pd.read_table(filename, names=['src', 'tgt'])\n",
    "        #print(\"Number of samples used from \"+filename+\": \",str(len(lines)) )\n",
    "    elif isinstance(filename,tuple):\n",
    "        s,t = load_two_files(*filename) # first is src, second is tgt.\n",
    "        lines = pd.DataFrame({'src':s, 'tgt':t})\n",
    "        #print(\"Number of samples used from \"+\", \".join(filename)+\": \",str(len(lines)) )\n",
    "    else:\n",
    "        raise ValueError(\"Must be either name of the file or a tuple of filenames (src, tgt).\")\n",
    "\n",
    "    if numlines is not None:\n",
    "        lines = lines[:numlines]\n",
    "\n",
    "\n",
    "    if optional_processing:\n",
    "        lines.src=lines.src.apply(lambda x: x.lower())\n",
    "        lines.tgt=lines.tgt.apply(lambda x: x.lower())\n",
    "        #lines.src=lines.src.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "        #lines.tgt=lines.tgt.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "        exclude = set(string.punctuation)\n",
    "        lines.src=lines.src.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "        lines.tgt=lines.tgt.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "    \n",
    "    if revert_tgt:\n",
    "        lines.tgt = lines.tgt.apply(lambda x : \" \".join(x.split()[::-1]))\n",
    "    \n",
    "    if drop_cls:\n",
    "        lines.src = lines.src.apply(lambda x : x.replace(\"<CLS> \", \"\"))\n",
    "\n",
    "    lines.tgt = lines.tgt.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def load_two_files(src_file, tgt_file):\n",
    "    with open(src_file,'r', encoding='utf8') as fd:\n",
    "        src = fd.readlines()\n",
    "    src = [i.strip() for i in src]\n",
    "\n",
    "    with open(tgt_file,'r', encoding='utf8') as fd:\n",
    "        tgt = fd.readlines()\n",
    "    tgt = [i.strip() for i in tgt]\n",
    "\n",
    "    return src, tgt\n",
    "\n",
    "def get_max_sentence_lengths(lines):\n",
    "    max_target_sentence = max([len(i.split()) for i in lines.tgt])\n",
    "    max_source_sentence = max([len(i.split()) for i in lines.src])\n",
    "    print(\"Max sequence length for inputs: \", max_source_sentence)\n",
    "    print(\"Max sequence length for outputs: \", max_target_sentence)\n",
    "    return max_source_sentence, max_target_sentence\n",
    "\n",
    "\n",
    "def prepare_data_shared(lines):\n",
    "    all_src_words=set()\n",
    "    for src in lines.src:\n",
    "        for word in src.split():\n",
    "            if word not in all_src_words:\n",
    "                all_src_words.add(word)\n",
    "\n",
    "    all_tgt_words=set()\n",
    "    for tgt in lines.tgt:\n",
    "        for word in tgt.split():\n",
    "            if word not in all_tgt_words:\n",
    "                all_tgt_words.add(word)\n",
    "\n",
    "    all_words = all_src_words | all_tgt_words\n",
    "    all_words = all_words - {'START_'}\n",
    "    #all_tgt_words = all_tgt_words - {'START_'}\n",
    "\n",
    "    #input_words = sorted(list(all_src_words))\n",
    "    target_words = ['START_']+sorted(list(all_words)) # want 'START_' to be the first\n",
    "\n",
    "    #NOTE: want the first entry (0th) to correspond to the start symbol\n",
    "    #input_w2i = dict(\n",
    "    #    [(word, i) for i, word in enumerate(input_words)])\n",
    "    target_w2i = dict(\n",
    "        [(word, i) for i, word in enumerate(target_words)])\n",
    "\n",
    "    input_w2i = target_w2i\n",
    "\n",
    "    print(\"Target vocab size: \", len(target_w2i))\n",
    "    print(\"Source vocab size: \", len(input_w2i))\n",
    "    return input_w2i, target_w2i\n",
    "\n",
    "\n",
    "def prepare_data(lines):\n",
    "    all_src_words=set()\n",
    "    for src in lines.src:\n",
    "        for word in src.split():\n",
    "            if word not in all_src_words:\n",
    "                all_src_words.add(word)\n",
    "\n",
    "    all_tgt_words=set()\n",
    "    for tgt in lines.tgt:\n",
    "        for word in tgt.split():\n",
    "            if word not in all_tgt_words:\n",
    "                all_tgt_words.add(word)\n",
    "    all_tgt_words = all_tgt_words - {'START_'}\n",
    "\n",
    "    input_words = [\"PAD\"]+sorted(list(all_src_words))\n",
    "    target_words = [\"PAD\", 'START_']+sorted(list(all_tgt_words))\n",
    "\n",
    "    #NOTE: want the first entry (0th) to correspond to the start symbol\n",
    "    input_w2i = dict(\n",
    "        [(word, i) for i, word in enumerate(input_words)])\n",
    "    target_w2i = dict(\n",
    "        [(word, i) for i, word in enumerate(target_words)])\n",
    "\n",
    "    print(\"Source vocab size: \", len(input_w2i))\n",
    "    print(\"Target vocab size: \", len(target_w2i))\n",
    "\n",
    "    return input_w2i, target_w2i\n",
    "\n",
    "\n",
    "def encode_texts(input_texts, input_w2i, max_source_sentence):\n",
    "\n",
    "    ##max_source_sentence = max([len(i.split()) for i in input_texts])\n",
    "\n",
    "    encoder_input = np.zeros(\n",
    "        (len(input_texts), max_source_sentence),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            encoder_input[i, t] = input_w2i[word] #TODO get keyerror now.. what about OOV?\n",
    "\n",
    "    return encoder_input\n",
    "\n",
    "\n",
    "def get_encoder_and_decoder_arrays(input_w2i, target_w2i, max_source_sentence, max_target_sentence, lines):\n",
    "\n",
    "    source_i2w = dict((i, word) for word,i in input_w2i.items())\n",
    "    target_i2w = dict((i, word) for word,i in target_w2i.items())\n",
    "\n",
    "    num_decoder_tokens = len(target_w2i)\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(lines.src), max_source_sentence),\n",
    "        dtype='float32')\n",
    "\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(lines.tgt), max_target_sentence),\n",
    "        dtype='float32')\n",
    "\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(lines.tgt), max_target_sentence, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(lines.src, lines.tgt)):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            encoder_input_data[i, t] = input_w2i[word]\n",
    "\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            decoder_input_data[i, t] = target_w2i[word]\n",
    "            if t > 0:\n",
    "                # Teacher forcing.\n",
    "                # Decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, target_w2i[word]] = 1. # probability=1 on the known word.\n",
    "\n",
    "\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-18 15:49:39--  https://www.dropbox.com/s/z6kie6b3awsb5wc/WN18RR-hp.zip?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.71.18, 2620:100:6026:18::a27d:4612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.71.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/z6kie6b3awsb5wc/WN18RR-hp.zip [following]\n",
      "--2021-02-18 15:49:40--  https://www.dropbox.com/s/raw/z6kie6b3awsb5wc/WN18RR-hp.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com/cd/0/inline/BJKwyn_1_ZrPie8NoebNGxC7TKiNfaJ8_sPp2cSTSZ8Zq0tY4PLPhUMc61BjXOS0n5UsuQITcIQkWgXOCSJWRoOM0NWCmKASP8riW_Ya_Gkvzf1mB8t0eKK0NLx4ZiHbFug/file# [following]\n",
      "--2021-02-18 15:49:41--  https://uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com/cd/0/inline/BJKwyn_1_ZrPie8NoebNGxC7TKiNfaJ8_sPp2cSTSZ8Zq0tY4PLPhUMc61BjXOS0n5UsuQITcIQkWgXOCSJWRoOM0NWCmKASP8riW_Ya_Gkvzf1mB8t0eKK0NLx4ZiHbFug/file\n",
      "Resolving uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com (uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com)... 162.125.71.15, 2620:100:6026:15::a27d:460f\n",
      "Connecting to uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com (uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com)|162.125.71.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BJLwWPK2BsRsRBl1IzBoL16LYg6PJ32eFrPLg6q2TfwR5xMN-8upbeuEXm8NLJ8V0dvmPA8MwhdfigFKZPAdrMhNUlR1kp16-Nu_lDNWYemFbZfLt449-ngiWai6BF8kLnEhNt13cHqqYYwxOWOrEBPY71aqJqVVse0xLFy541oaRpJJURelQKoQAxxSQgADzzOsNvAJFSMd6O4bP-hciNGUXdWjv5yC9JUHN5y5b2zS83nJsrCieE5bwEghGFQIEbbcHxDbQ_ehKy3ncpt-lCzwSXRax5UGHPSiz4lLZSqmZLw192T1yT6vqemXJHEMBOeEB4y67nQkTwikAMun5daYeahd7XC_z5l2X9jsBo9BwA/file [following]\n",
      "--2021-02-18 15:49:42--  https://uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com/cd/0/inline2/BJLwWPK2BsRsRBl1IzBoL16LYg6PJ32eFrPLg6q2TfwR5xMN-8upbeuEXm8NLJ8V0dvmPA8MwhdfigFKZPAdrMhNUlR1kp16-Nu_lDNWYemFbZfLt449-ngiWai6BF8kLnEhNt13cHqqYYwxOWOrEBPY71aqJqVVse0xLFy541oaRpJJURelQKoQAxxSQgADzzOsNvAJFSMd6O4bP-hciNGUXdWjv5yC9JUHN5y5b2zS83nJsrCieE5bwEghGFQIEbbcHxDbQ_ehKy3ncpt-lCzwSXRax5UGHPSiz4lLZSqmZLw192T1yT6vqemXJHEMBOeEB4y67nQkTwikAMun5daYeahd7XC_z5l2X9jsBo9BwA/file\n",
      "Reusing existing connection to uc31d7ab763a8afe0decd3a4fd51.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 133519137 (127M) [application/zip]\n",
      "Saving to: ‘WN18RR-hp.zip’\n",
      "\n",
      "WN18RR-hp.zip       100%[===================>] 127.33M  22.0MB/s    in 6.8s    \n",
      "\n",
      "2021-02-18 15:49:50 (18.8 MB/s) - ‘WN18RR-hp.zip’ saved [133519137/133519137]\n",
      "\n",
      "/bin/sh: 1: unzip: not found\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/z6kie6b3awsb5wc/WN18RR-hp.zip?dl=0 -O WN18RR-hp.zip\n",
    "!unzip WN18RR-hp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path_src = \"./WN18RR-hp/noun/src-path-train-lemmas.txt\"\n",
    "training_data_path_tgt = \"./WN18RR-hp/noun/tgt-path-train.txt\" \n",
    "validation_data_path_src = \"./WN18RR-hp/noun/src-path-valid-lemmas.txt\" \n",
    "validation_data_path_tgt = \"./WN18RR-hp/noun/tgt-path-valid.txt\" \n",
    "test_data_path_src = \"./WN18RR-hp/noun/src-path-test-lemmas.txt\" \n",
    "test_data_path_tgt = \"./WN18RR-hp/noun/tgt-path-test.txt\" \n",
    "src_emb_file = \"./WN18RR-hp/all_lemmas_fasttext_cased.txt\" \n",
    "tgt_emb_file = \"./colab_data/ft-embs-all-lower.vec\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "num_samples_train = None\n",
    "num_samples_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>land reform</td>\n",
       "      <td>START_ reform.n.01 improvement.n.02 change_of_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empathy</td>\n",
       "      <td>START_ sympathy.n.02 feeling.n.01 state.n.02 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disengagement &lt;CLS&gt; fallback &lt;CLS&gt; pullout</td>\n",
       "      <td>START_ retreat.n.01 withdrawal.n.03 departure....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Equisetum &lt;CLS&gt; genus Equisetum</td>\n",
       "      <td>START_ fern_genus.n.01 genus.n.02 taxonomic_gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queen &lt;CLS&gt; queen regnant &lt;CLS&gt; female monarch</td>\n",
       "      <td>START_ female_aristocrat.n.01 aristocrat.n.01 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              src  \\\n",
       "0                                     land reform   \n",
       "1                                         empathy   \n",
       "2      disengagement <CLS> fallback <CLS> pullout   \n",
       "3                 Equisetum <CLS> genus Equisetum   \n",
       "4  queen <CLS> queen regnant <CLS> female monarch   \n",
       "\n",
       "                                                 tgt  \n",
       "0  START_ reform.n.01 improvement.n.02 change_of_...  \n",
       "1  START_ sympathy.n.02 feeling.n.01 state.n.02 a...  \n",
       "2  START_ retreat.n.01 withdrawal.n.03 departure....  \n",
       "3  START_ fern_genus.n.01 genus.n.02 taxonomic_gr...  \n",
       "4  START_ female_aristocrat.n.01 aristocrat.n.01 ...  "
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_data((training_data_path_src, training_data_path_tgt), numlines=num_samples_train, revert_tgt=True, drop_cls=False)\n",
    "val_data = load_data((validation_data_path_src, validation_data_path_tgt), numlines=num_samples_val, revert_tgt=True, drop_cls=False)\n",
    "test_data = load_data((test_data_path_src, test_data_path_tgt), numlines=num_samples_val, revert_tgt=True, drop_cls=False)\n",
    "\n",
    "train_and_val_and_test = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "train_and_val_and_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length for inputs:  58\n",
      "Max sequence length for outputs:  20\n"
     ]
    }
   ],
   "source": [
    "max_source_sentence_length, max_target_sentence_length = get_max_sentence_lengths(train_and_val_and_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size:  36923\n",
      "Target vocab size:  7902\n"
     ]
    }
   ],
   "source": [
    "input_w2i, target_w2i = prepare_data(train_and_val_and_test)\n",
    "target_i2w = dict((i, word) for word,i in target_w2i.items())\n",
    "# Only need this if using pretrained embeddings for encoder.\n",
    "input_i2w = dict((i, word) for word,i in input_w2i.items())\n",
    "\n",
    "num_unique_input_chars = len(input_w2i)\n",
    "num_unique_target_chars = len(target_w2i)\n",
    "\n",
    "if not os.path.exists('./word_models'):\n",
    "        os.makedirs('./word_models')\n",
    "\n",
    "if src_emb_file is not None:\n",
    "    src_w2v = load_embs(src_emb_file)\n",
    "    emb_matrix_src, src_OOV = get_emb_matrix(input_i2w, src_w2v, embedding_size, with_flags=False, oov_filename='word_models/src_oov.txt')\n",
    "else:\n",
    "    emb_matrix_src = None\n",
    "\n",
    "if tgt_emb_file is not None:\n",
    "    tgt_w2v = load_embs(tgt_emb_file)\n",
    "    emb_matrix_tgt, tgt_OOV = get_emb_matrix(target_i2w, tgt_w2v, embedding_size, with_flags=True, oov_filename='word_models/tgt_oov.txt')\n",
    "else:\n",
    "    emb_matrix_tgt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_to_ind_arrays(input_w2i, target_w2i, max_source_sentence, max_target_sentence, lines):\n",
    "\n",
    "    source_i2w = dict((i, word) for word,i in input_w2i.items())\n",
    "    target_i2w = dict((i, word) for word,i in target_w2i.items())\n",
    "\n",
    "    num_decoder_tokens = len(target_w2i)\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(lines.src), max_source_sentence),\n",
    "        dtype='long')\n",
    "\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(lines.tgt), max_target_sentence),\n",
    "        dtype='long')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(lines.src, lines.tgt)):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            encoder_input_data[i, t] = input_w2i[word]\n",
    "\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            decoder_input_data[i, t] = target_w2i[word]\n",
    "\n",
    "    return encoder_input_data, decoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = get_token_to_ind_arrays(input_w2i, target_w2i, max_source_sentence_length, max_target_sentence_length, train_data)\n",
    "x_val, y_val = get_token_to_ind_arrays(input_w2i, target_w2i, max_source_sentence_length, max_target_sentence_length, val_data)\n",
    "x_test, y_test = get_token_to_ind_arrays(input_w2i, target_w2i, max_source_sentence_length, max_target_sentence_length, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_examples = tf.data.Dataset.from_tensor_slices((x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build an input pipeline suitable for training you'll apply some transformations to the dataset.\n",
    "\n",
    "This function will be used to encode the batches of raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(in_, out_):\n",
    "    return in_, out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "    \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, emb_matrix_src=None, trainable=False,rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if emb_matrix_src is None:\n",
    "            self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, \n",
    "                                                       embeddings_initializer=tf.keras.initializers.Constant(emb_matrix_src),\n",
    "                                                       trainable=trainable)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, emb_matrix_tgt=None, trainable=False, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if emb_matrix_tgt is None:\n",
    "            self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, \n",
    "                                                       embeddings_initializer=tf.keras.initializers.Constant(emb_matrix_tgt),\n",
    "                                                       trainable=trainable)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "      \n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, emb_matrix_src=None, emb_matrix_tgt=None,\n",
    "               trainable_enc=False, trainable_dec=False, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, emb_matrix_src, trainable_enc, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, emb_matrix_tgt, trainable_dec, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "d_model = 300\n",
    "dff = 512\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=len(input_w2i),\n",
    "    target_vocab_size=len(target_w2i), \n",
    "    pe_input=1000, \n",
    "    pe_target=1000,\n",
    "    emb_matrix_src=emb_matrix_src,\n",
    "    emb_matrix_tgt=emb_matrix_tgt,\n",
    "    trainable_enc=False,\n",
    "    trainable_dec=False,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "#if ckpt_manager.latest_checkpoint:\n",
    "    #ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    #print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.9900 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.8203 Accuracy 0.0313\n",
      "Epoch 1 Batch 100 Loss 8.5285 Accuracy 0.0757\n",
      "Epoch 1 Batch 150 Loss 8.2381 Accuracy 0.1036\n",
      "Epoch 1 Batch 200 Loss 7.9159 Accuracy 0.1314\n",
      "Epoch 1 Batch 250 Loss 7.5370 Accuracy 0.1552\n",
      "Epoch 1 Batch 300 Loss 7.1361 Accuracy 0.1950\n",
      "Epoch 1 Batch 350 Loss 6.7371 Accuracy 0.2366\n",
      "Epoch 1 Batch 400 Loss 6.3612 Accuracy 0.2737\n",
      "Epoch 1 Batch 450 Loss 6.0167 Accuracy 0.3073\n",
      "Epoch 1 Batch 500 Loss 5.7081 Accuracy 0.3372\n",
      "Epoch 1 Train Loss 5.4800 Train Accuracy 0.3592\n",
      "Epoch 1 Val Loss 2.5345 Val Accuracy 0.6506\n",
      "Time taken for 1 epoch: 355.24 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-1\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6714 Accuracy 0.6375\n",
      "Epoch 2 Batch 50 Loss 2.4920 Accuracy 0.6497\n",
      "Epoch 2 Batch 100 Loss 2.3996 Accuracy 0.6601\n",
      "Epoch 2 Batch 150 Loss 2.3262 Accuracy 0.6684\n",
      "Epoch 2 Batch 200 Loss 2.2609 Accuracy 0.6756\n",
      "Epoch 2 Batch 250 Loss 2.1982 Accuracy 0.6831\n",
      "Epoch 2 Batch 300 Loss 2.1456 Accuracy 0.6897\n",
      "Epoch 2 Batch 350 Loss 2.0915 Accuracy 0.6963\n",
      "Epoch 2 Batch 400 Loss 2.0428 Accuracy 0.7025\n",
      "Epoch 2 Batch 450 Loss 1.9953 Accuracy 0.7084\n",
      "Epoch 2 Batch 500 Loss 1.9493 Accuracy 0.7141\n",
      "Epoch 2 Train Loss 1.9184 Train Accuracy 0.7178\n",
      "Epoch 2 Val Loss 1.5120 Val Accuracy 0.7717\n",
      "Time taken for 1 epoch: 337.73 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-2\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.5855 Accuracy 0.7560\n",
      "Epoch 3 Batch 50 Loss 1.4242 Accuracy 0.7745\n",
      "Epoch 3 Batch 100 Loss 1.4035 Accuracy 0.7763\n",
      "Epoch 3 Batch 150 Loss 1.3824 Accuracy 0.7800\n",
      "Epoch 3 Batch 200 Loss 1.3655 Accuracy 0.7831\n",
      "Epoch 3 Batch 250 Loss 1.3465 Accuracy 0.7855\n",
      "Epoch 3 Batch 300 Loss 1.3282 Accuracy 0.7879\n",
      "Epoch 3 Batch 350 Loss 1.3132 Accuracy 0.7902\n",
      "Epoch 3 Batch 400 Loss 1.2969 Accuracy 0.7924\n",
      "Epoch 3 Batch 450 Loss 1.2803 Accuracy 0.7948\n",
      "Epoch 3 Batch 500 Loss 1.2645 Accuracy 0.7971\n",
      "Epoch 3 Train Loss 1.2543 Train Accuracy 0.7984\n",
      "Epoch 3 Val Loss 1.1507 Val Accuracy 0.8175\n",
      "Time taken for 1 epoch: 339.34 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-3\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.0348 Accuracy 0.8352\n",
      "Epoch 4 Batch 50 Loss 1.0342 Accuracy 0.8243\n",
      "Epoch 4 Batch 100 Loss 1.0267 Accuracy 0.8244\n",
      "Epoch 4 Batch 150 Loss 1.0113 Accuracy 0.8267\n",
      "Epoch 4 Batch 200 Loss 1.0029 Accuracy 0.8289\n",
      "Epoch 4 Batch 250 Loss 1.0019 Accuracy 0.8294\n",
      "Epoch 4 Batch 300 Loss 0.9976 Accuracy 0.8301\n",
      "Epoch 4 Batch 350 Loss 0.9923 Accuracy 0.8310\n",
      "Epoch 4 Batch 400 Loss 0.9837 Accuracy 0.8321\n",
      "Epoch 4 Batch 450 Loss 0.9765 Accuracy 0.8330\n",
      "Epoch 4 Batch 500 Loss 0.9713 Accuracy 0.8337\n",
      "Epoch 4 Train Loss 0.9657 Train Accuracy 0.8346\n",
      "Epoch 4 Val Loss 0.9986 Val Accuracy 0.8382\n",
      "Time taken for 1 epoch: 337.92 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-4\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.8063 Accuracy 0.8389\n",
      "Epoch 5 Batch 50 Loss 0.8335 Accuracy 0.8487\n",
      "Epoch 5 Batch 100 Loss 0.8194 Accuracy 0.8510\n",
      "Epoch 5 Batch 150 Loss 0.8155 Accuracy 0.8521\n",
      "Epoch 5 Batch 200 Loss 0.8183 Accuracy 0.8521\n",
      "Epoch 5 Batch 250 Loss 0.8215 Accuracy 0.8517\n",
      "Epoch 5 Batch 300 Loss 0.8200 Accuracy 0.8519\n",
      "Epoch 5 Batch 350 Loss 0.8182 Accuracy 0.8525\n",
      "Epoch 5 Batch 400 Loss 0.8176 Accuracy 0.8525\n",
      "Epoch 5 Batch 450 Loss 0.8175 Accuracy 0.8529\n",
      "Epoch 5 Batch 500 Loss 0.8138 Accuracy 0.8533\n",
      "Epoch 5 Train Loss 0.8107 Train Accuracy 0.8538\n",
      "Epoch 5 Val Loss 0.9191 Val Accuracy 0.8510\n",
      "Time taken for 1 epoch: 343.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.6537 Accuracy 0.8664\n",
      "Epoch 6 Batch 50 Loss 0.6918 Accuracy 0.8687\n",
      "Epoch 6 Batch 100 Loss 0.7115 Accuracy 0.8661\n",
      "Epoch 6 Batch 150 Loss 0.7169 Accuracy 0.8651\n",
      "Epoch 6 Batch 200 Loss 0.7192 Accuracy 0.8649\n",
      "Epoch 6 Batch 250 Loss 0.7178 Accuracy 0.8654\n",
      "Epoch 6 Batch 300 Loss 0.7166 Accuracy 0.8656\n",
      "Epoch 6 Batch 350 Loss 0.7190 Accuracy 0.8652\n",
      "Epoch 6 Batch 400 Loss 0.7196 Accuracy 0.8654\n",
      "Epoch 6 Batch 450 Loss 0.7197 Accuracy 0.8654\n",
      "Epoch 6 Batch 500 Loss 0.7202 Accuracy 0.8656\n",
      "Epoch 6 Train Loss 0.7219 Train Accuracy 0.8655\n",
      "Epoch 6 Val Loss 0.8897 Val Accuracy 0.8538\n",
      "Time taken for 1 epoch: 344.53 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-6\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.6200 Accuracy 0.8742\n",
      "Epoch 7 Batch 50 Loss 0.6528 Accuracy 0.8708\n",
      "Epoch 7 Batch 100 Loss 0.6561 Accuracy 0.8717\n",
      "Epoch 7 Batch 150 Loss 0.6584 Accuracy 0.8719\n",
      "Epoch 7 Batch 200 Loss 0.6563 Accuracy 0.8721\n",
      "Epoch 7 Batch 250 Loss 0.6554 Accuracy 0.8725\n",
      "Epoch 7 Batch 300 Loss 0.6569 Accuracy 0.8724\n",
      "Epoch 7 Batch 350 Loss 0.6604 Accuracy 0.8722\n",
      "Epoch 7 Batch 400 Loss 0.6625 Accuracy 0.8720\n",
      "Epoch 7 Batch 450 Loss 0.6636 Accuracy 0.8719\n",
      "Epoch 7 Batch 500 Loss 0.6656 Accuracy 0.8717\n",
      "Epoch 7 Train Loss 0.6675 Train Accuracy 0.8714\n",
      "Epoch 7 Val Loss 0.8719 Val Accuracy 0.8596\n",
      "Time taken for 1 epoch: 345.88 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-7\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6089 Accuracy 0.8784\n",
      "Epoch 8 Batch 50 Loss 0.5907 Accuracy 0.8819\n",
      "Epoch 8 Batch 100 Loss 0.6028 Accuracy 0.8785\n",
      "Epoch 8 Batch 150 Loss 0.6067 Accuracy 0.8781\n",
      "Epoch 8 Batch 200 Loss 0.6111 Accuracy 0.8772\n",
      "Epoch 8 Batch 250 Loss 0.6173 Accuracy 0.8765\n",
      "Epoch 8 Batch 300 Loss 0.6223 Accuracy 0.8762\n",
      "Epoch 8 Batch 350 Loss 0.6239 Accuracy 0.8760\n",
      "Epoch 8 Batch 400 Loss 0.6279 Accuracy 0.8758\n",
      "Epoch 8 Batch 450 Loss 0.6296 Accuracy 0.8758\n",
      "Epoch 8 Batch 500 Loss 0.6312 Accuracy 0.8757\n",
      "Epoch 8 Train Loss 0.6335 Train Accuracy 0.8754\n",
      "Epoch 8 Val Loss 0.8509 Val Accuracy 0.8635\n",
      "Time taken for 1 epoch: 340.59 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-8\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5209 Accuracy 0.8794\n",
      "Epoch 9 Batch 50 Loss 0.5531 Accuracy 0.8827\n",
      "Epoch 9 Batch 100 Loss 0.5542 Accuracy 0.8832\n",
      "Epoch 9 Batch 150 Loss 0.5602 Accuracy 0.8827\n",
      "Epoch 9 Batch 200 Loss 0.5635 Accuracy 0.8830\n",
      "Epoch 9 Batch 250 Loss 0.5683 Accuracy 0.8827\n",
      "Epoch 9 Batch 300 Loss 0.5705 Accuracy 0.8826\n",
      "Epoch 9 Batch 350 Loss 0.5739 Accuracy 0.8826\n",
      "Epoch 9 Batch 400 Loss 0.5769 Accuracy 0.8827\n",
      "Epoch 9 Batch 450 Loss 0.5780 Accuracy 0.8829\n",
      "Epoch 9 Batch 500 Loss 0.5792 Accuracy 0.8827\n",
      "Epoch 9 Train Loss 0.5824 Train Accuracy 0.8825\n",
      "Epoch 9 Val Loss 0.8437 Val Accuracy 0.8642\n",
      "Time taken for 1 epoch: 341.14 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-9\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4156 Accuracy 0.9073\n",
      "Epoch 10 Batch 50 Loss 0.5045 Accuracy 0.8921\n",
      "Epoch 10 Batch 100 Loss 0.5054 Accuracy 0.8927\n",
      "Epoch 10 Batch 150 Loss 0.5081 Accuracy 0.8916\n",
      "Epoch 10 Batch 200 Loss 0.5132 Accuracy 0.8910\n",
      "Epoch 10 Batch 250 Loss 0.5192 Accuracy 0.8904\n",
      "Epoch 10 Batch 300 Loss 0.5224 Accuracy 0.8902\n",
      "Epoch 10 Batch 350 Loss 0.5274 Accuracy 0.8896\n",
      "Epoch 10 Batch 400 Loss 0.5302 Accuracy 0.8894\n",
      "Epoch 10 Batch 450 Loss 0.5333 Accuracy 0.8893\n",
      "Epoch 10 Batch 500 Loss 0.5367 Accuracy 0.8890\n",
      "Epoch 10 Train Loss 0.5390 Train Accuracy 0.8888\n",
      "Epoch 10 Val Loss 0.8366 Val Accuracy 0.8696\n",
      "Time taken for 1 epoch: 341.61 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-10\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5136 Accuracy 0.8950\n",
      "Epoch 11 Batch 50 Loss 0.4581 Accuracy 0.8995\n",
      "Epoch 11 Batch 100 Loss 0.4684 Accuracy 0.8970\n",
      "Epoch 11 Batch 150 Loss 0.4692 Accuracy 0.8975\n",
      "Epoch 11 Batch 200 Loss 0.4770 Accuracy 0.8965\n",
      "Epoch 11 Batch 250 Loss 0.4803 Accuracy 0.8963\n",
      "Epoch 11 Batch 300 Loss 0.4880 Accuracy 0.8952\n",
      "Epoch 11 Batch 350 Loss 0.4915 Accuracy 0.8950\n",
      "Epoch 11 Batch 400 Loss 0.4948 Accuracy 0.8945\n",
      "Epoch 11 Batch 450 Loss 0.4982 Accuracy 0.8942\n",
      "Epoch 11 Batch 500 Loss 0.5020 Accuracy 0.8940\n",
      "Epoch 11 Train Loss 0.5038 Train Accuracy 0.8938\n",
      "Epoch 11 Val Loss 0.8537 Val Accuracy 0.8674\n",
      "Time taken for 1 epoch: 343.65 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/train/ckpt-11\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4485 Accuracy 0.9024\n",
      "Epoch 12 Batch 50 Loss 0.4377 Accuracy 0.9014\n",
      "Epoch 12 Batch 100 Loss 0.4358 Accuracy 0.9015\n",
      "Epoch 12 Batch 150 Loss 0.4392 Accuracy 0.9013\n",
      "Epoch 12 Batch 200 Loss 0.4446 Accuracy 0.9005\n",
      "Epoch 12 Batch 250 Loss 0.4462 Accuracy 0.9003\n",
      "Epoch 12 Batch 300 Loss 0.4511 Accuracy 0.8998\n",
      "Epoch 12 Batch 350 Loss 0.4563 Accuracy 0.8994\n",
      "Epoch 12 Batch 400 Loss 0.4605 Accuracy 0.8991\n",
      "Epoch 12 Batch 450 Loss 0.4647 Accuracy 0.8988\n",
      "Epoch 12 Batch 500 Loss 0.4685 Accuracy 0.8985\n",
      "Epoch 12 Train Loss 0.4714 Train Accuracy 0.8983\n",
      "Epoch 12 Val Loss 0.8485 Val Accuracy 0.8705\n",
      "Time taken for 1 epoch: 344.74 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/train/ckpt-12\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.4104 Accuracy 0.9234\n",
      "Epoch 13 Batch 50 Loss 0.3988 Accuracy 0.9085\n",
      "Epoch 13 Batch 100 Loss 0.4051 Accuracy 0.9067\n",
      "Epoch 13 Batch 150 Loss 0.4098 Accuracy 0.9068\n",
      "Epoch 13 Batch 200 Loss 0.4149 Accuracy 0.9058\n",
      "Epoch 13 Batch 250 Loss 0.4215 Accuracy 0.9049\n",
      "Epoch 13 Batch 300 Loss 0.4274 Accuracy 0.9041\n",
      "Epoch 13 Batch 350 Loss 0.4325 Accuracy 0.9036\n",
      "Epoch 13 Batch 400 Loss 0.4367 Accuracy 0.9029\n",
      "Epoch 13 Batch 450 Loss 0.4412 Accuracy 0.9025\n",
      "Epoch 13 Batch 500 Loss 0.4452 Accuracy 0.9023\n",
      "Epoch 13 Train Loss 0.4483 Train Accuracy 0.9018\n",
      "Epoch 13 Val Loss 0.8699 Val Accuracy 0.8713\n",
      "Time taken for 1 epoch: 343.86 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/train/ckpt-13\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.3406 Accuracy 0.9036\n",
      "Epoch 14 Batch 50 Loss 0.3772 Accuracy 0.9112\n",
      "Epoch 14 Batch 100 Loss 0.3809 Accuracy 0.9110\n",
      "Epoch 14 Batch 150 Loss 0.3889 Accuracy 0.9101\n",
      "Epoch 14 Batch 200 Loss 0.3935 Accuracy 0.9094\n",
      "Epoch 14 Batch 250 Loss 0.3975 Accuracy 0.9090\n",
      "Epoch 14 Batch 300 Loss 0.4016 Accuracy 0.9085\n",
      "Epoch 14 Batch 350 Loss 0.4059 Accuracy 0.9080\n",
      "Epoch 14 Batch 400 Loss 0.4091 Accuracy 0.9076\n",
      "Epoch 14 Batch 450 Loss 0.4128 Accuracy 0.9073\n",
      "Epoch 14 Batch 500 Loss 0.4168 Accuracy 0.9067\n",
      "Epoch 14 Train Loss 0.4199 Train Accuracy 0.9063\n",
      "Epoch 14 Val Loss 0.8644 Val Accuracy 0.8721\n",
      "Time taken for 1 epoch: 342.12 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/train/ckpt-14\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.3440 Accuracy 0.9211\n",
      "Epoch 15 Batch 50 Loss 0.3561 Accuracy 0.9164\n",
      "Epoch 15 Batch 100 Loss 0.3635 Accuracy 0.9148\n",
      "Epoch 15 Batch 150 Loss 0.3711 Accuracy 0.9133\n",
      "Epoch 15 Batch 200 Loss 0.3749 Accuracy 0.9129\n",
      "Epoch 15 Batch 250 Loss 0.3767 Accuracy 0.9124\n",
      "Epoch 15 Batch 300 Loss 0.3824 Accuracy 0.9116\n",
      "Epoch 15 Batch 350 Loss 0.3861 Accuracy 0.9111\n",
      "Epoch 15 Batch 400 Loss 0.3902 Accuracy 0.9105\n",
      "Epoch 15 Batch 450 Loss 0.3953 Accuracy 0.9099\n",
      "Epoch 15 Batch 500 Loss 0.3992 Accuracy 0.9095\n",
      "Epoch 15 Train Loss 0.4031 Train Accuracy 0.9091\n",
      "Epoch 15 Val Loss 0.8588 Val Accuracy 0.8744\n",
      "Time taken for 1 epoch: 341.03 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-15\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3584 Accuracy 0.9262\n",
      "Epoch 16 Batch 50 Loss 0.3379 Accuracy 0.9160\n",
      "Epoch 16 Batch 100 Loss 0.3429 Accuracy 0.9166\n",
      "Epoch 16 Batch 150 Loss 0.3473 Accuracy 0.9163\n",
      "Epoch 16 Batch 200 Loss 0.3515 Accuracy 0.9155\n",
      "Epoch 16 Batch 250 Loss 0.3564 Accuracy 0.9151\n",
      "Epoch 16 Batch 300 Loss 0.3642 Accuracy 0.9142\n",
      "Epoch 16 Batch 350 Loss 0.3679 Accuracy 0.9137\n",
      "Epoch 16 Batch 400 Loss 0.3720 Accuracy 0.9132\n",
      "Epoch 16 Batch 450 Loss 0.3766 Accuracy 0.9129\n",
      "Epoch 16 Batch 500 Loss 0.3808 Accuracy 0.9124\n",
      "Epoch 16 Train Loss 0.3835 Train Accuracy 0.9122\n",
      "Epoch 16 Val Loss 0.8766 Val Accuracy 0.8752\n",
      "Time taken for 1 epoch: 337.65 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/train/ckpt-16\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.3481 Accuracy 0.9202\n",
      "Epoch 17 Batch 50 Loss 0.3210 Accuracy 0.9192\n",
      "Epoch 17 Batch 100 Loss 0.3355 Accuracy 0.9176\n",
      "Epoch 17 Batch 150 Loss 0.3374 Accuracy 0.9172\n",
      "Epoch 17 Batch 200 Loss 0.3422 Accuracy 0.9168\n",
      "Epoch 17 Batch 250 Loss 0.3452 Accuracy 0.9164\n",
      "Epoch 17 Batch 300 Loss 0.3479 Accuracy 0.9162\n",
      "Epoch 17 Batch 350 Loss 0.3524 Accuracy 0.9158\n",
      "Epoch 17 Batch 400 Loss 0.3553 Accuracy 0.9155\n",
      "Epoch 17 Batch 450 Loss 0.3594 Accuracy 0.9152\n",
      "Epoch 17 Batch 500 Loss 0.3641 Accuracy 0.9146\n",
      "Epoch 17 Train Loss 0.3686 Train Accuracy 0.9140\n",
      "Epoch 17 Val Loss 0.8870 Val Accuracy 0.8765\n",
      "Time taken for 1 epoch: 340.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/train/ckpt-17\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.3300 Accuracy 0.9207\n"
     ]
    }
   ],
   "source": [
    "prev_best_loss=10000\n",
    "\n",
    "with open('transformer_log.txt', \"w\") as logfile:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "            train_step(inp, tar)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                str0 = f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}'\n",
    "                print(str0)\n",
    "                logfile.write(str0)\n",
    "                logfile.write(\"\\n\")\n",
    "\n",
    "        new_loss = train_loss.result()\n",
    "        \n",
    "        val_losses=[]\n",
    "        val_accs=[]\n",
    "        \n",
    "        for (batch, (inp, tar)) in enumerate(val_batches):\n",
    "            tar_inp = tar[:, :-1]\n",
    "            tar_real = tar[:, 1:]\n",
    "\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "            predictions, _ = transformer(inp, tar_inp, \n",
    "                                 False, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "            new_val_loss = loss_function(tar_real, predictions)\n",
    "            new_val_acc = accuracy_function(tar_real, predictions)\n",
    "            \n",
    "            val_losses.append(new_val_loss.numpy())\n",
    "            val_accs.append(new_val_acc.numpy())\n",
    "\n",
    "        mean_val_loss=np.mean(val_losses)\n",
    "        mean_val_acc=np.mean(val_accs)\n",
    "        str1 = f'Epoch {epoch + 1} Train Loss {train_loss.result():.4f} Train Accuracy {train_accuracy.result():.4f}'\n",
    "        str2 = f'Epoch {epoch + 1} Val Loss {mean_val_loss:.4f} Val Accuracy {mean_val_acc:.4f}'\n",
    "        str3 = f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n'\n",
    "\n",
    "        print(str1)\n",
    "        print(str2)\n",
    "        print(str3)\n",
    "        \n",
    "        logfile.write(str1)\n",
    "        logfile.write(\"\\n\")\n",
    "        logfile.write(str2)\n",
    "        logfile.write(\"\\n\")\n",
    "        logfile.write(str3)\n",
    "        logfile.write(\"\\n\")\n",
    "        \n",
    "        if new_loss < prev_best_loss:\n",
    "            prev_best_loss = new_loss\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            str4 = f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\\n'\n",
    "            print(str4)\n",
    "            logfile.write(str4)\n",
    "            logfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/train/ckpt-8'"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb62322c390>"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(in_, max_length=20):\n",
    "    encoder_input = in_\n",
    "    encoder_input = tf.expand_dims(in_, 0)\n",
    "    output = tf.convert_to_tensor([1])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    output = tf.cast(output, tf.int64)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                    output,\n",
    "                                                    False,\n",
    "                                                    enc_padding_mask,\n",
    "                                                    combined_mask,\n",
    "                                                    dec_padding_mask)\n",
    "        \n",
    "        \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == 2:\n",
    "            break\n",
    "    \n",
    "    # output.shape (1, tokens)\n",
    "    tokens = [target_i2w[i] for i in output[0].numpy()] # shape: ()\n",
    "    return tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : ['nap', '<CLS>', 'catnap', '<CLS>', 'cat', 'sleep', '<CLS>', 'forty', 'winks', '<CLS>', 'short', 'sleep', '<CLS>', 'snooze', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'time_period.n.01', 'fundamental_quantity.n.01', 'measure.n.02', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'sleeping.n.03', 'bodily_process.n.01', 'organic_process.n.01', 'process.n.06', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['pension', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'proceeding.n.01', 'due_process.n.01', 'group_action.n.01', 'event.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'regular_payment.n.01', 'payment.n.01', 'cost.n.01', 'outgo.n.01', 'transferred_property.n.01', 'possession.n.02', 'relation.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['beater', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'mechanical_device.n.01', 'mechanism.n.05', 'device.n.01', 'instrumentality.n.03', 'artifact.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'implement.n.01', 'instrumentality.n.03', 'artifact.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['Papilionoideae', '<CLS>', 'subfamily', 'Papilionoideae', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'monocot_family.n.01', 'family.n.06', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'rosid_dicot_family.n.01', 'dicot_family.n.01', 'family.n.06', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['maxim', '<CLS>', 'axiom', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'doctrine.n.01', 'belief.n.01', 'content.n.05', 'cognition.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'saying.n.01', 'speech.n.02', 'auditory_communication.n.01', 'communication.n.02', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['indoctrination', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'defense_mechanism.n.01', 'process.n.04', 'cognition.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'teaching.n.01', 'education.n.04', 'profession.n.02', 'occupation.n.01', 'activity.n.01', 'act.n.02', 'event.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['kingbird', '<CLS>', 'Tyrannus', 'tyrannus', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'oscine.n.01', 'passerine.n.01', 'bird.n.01', 'vertebrate.n.01', 'chordate.n.01', 'animal.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'new_world_flycatcher.n.01', 'tyrannid.n.01', 'passerine.n.01', 'bird.n.01', 'vertebrate.n.01', 'chordate.n.01', 'animal.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['Rickettsiaceae', '<CLS>', 'family', 'Rickettsiaceae', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'protoctist_family.n.01', 'family.n.06', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'bacteria_family.n.01', 'family.n.06', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['Entoprocta', '<CLS>', 'phylum', 'Entoprocta', '<CLS>', 'Endoprocta', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'phylum.n.02', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'phylum.n.02', 'taxonomic_group.n.01', 'biological_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['breadfruit', '<CLS>', 'breadfruit', 'tree', '<CLS>', 'Artocarpus', 'communis', '<CLS>', 'Artocarpus', 'altilis', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'tree.n.01', 'woody_plant.n.01', 'vascular_plant.n.01', 'plant.n.02', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'fruit_tree.n.01', 'angiospermous_tree.n.01', 'tree.n.01', 'woody_plant.n.01', 'vascular_plant.n.01', 'plant.n.02', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['sympathizer', '<CLS>', 'sympathiser', '<CLS>', 'comforter', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'person.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'communicator.n.01', 'person.n.01', 'causal_agent.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['sympathizer', '<CLS>', 'sympathiser', '<CLS>', 'comforter', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'person.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'communicator.n.01', 'person.n.01', 'organism.n.01', 'living_thing.n.01', 'whole.n.02', 'object.n.01', 'physical_entity.n.01', 'entity.n.01', '_END']\n",
      "Input:         : ['accentuation', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'shape.n.01', 'spatial_property.n.01', 'property.n.02', 'attribute.n.02', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'stress.n.01', 'prosody.n.01', 'manner_of_speaking.n.01', 'expressive_style.n.01', 'communication.n.02', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['mayhem', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'conflict.n.01', 'group_action.n.01', 'event.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'crime.n.01', 'transgression.n.01', 'wrongdoing.n.02', 'activity.n.01', 'act.n.02', 'event.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Input:         : ['rail', 'technology', '<CLS>', 'railroading', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Prediction     : ['START_', 'railway.n.01', 'line.n.23', 'carrier.n.05', 'business.n.01', 'enterprise.n.02', 'organization.n.01', 'social_group.n.01', 'group.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n",
      "Ground truth   : ['START_', 'technology.n.01', 'application.n.01', 'use.n.01', 'activity.n.01', 'act.n.02', 'event.n.01', 'psychological_feature.n.01', 'abstraction.n.06', 'entity.n.01', '_END']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open(\"in_transformer_predictions.tsv\", 'w', encoding='utf-8') as f, open(\"out_transformer_predictions.tsv\", 'w', encoding='utf-8') as w:\n",
    "    for in_, out_ in test_examples:\n",
    "        translated_tokens, attention_weights = evaluate(in_)\n",
    "        f.write(input_i2w[in_.numpy()[0]]+\"\\n\")\n",
    "        w.write(\" \".join([i for i in translated_tokens if i!=\"START_\" and i!=\"_END\" and i!=\"PAD\"])+\"\\n\")\n",
    "        count += 1\n",
    "        if count  <= 15:\n",
    "            print_translation([input_i2w[i] for i in in_.numpy()], translated_tokens, [target_i2w[i] for i in out_.numpy() if i!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
